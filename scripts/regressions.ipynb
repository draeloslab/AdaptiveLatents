{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Sometimes the calculations in this notebook run faster if the GPU isn't enabled; I think this is especially true with some of the caching I do, so I usually run the cell below to have JAX run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:41:21.104905Z",
     "start_time": "2023-11-03T14:41:20.201483Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import adaptive_latents.input_sources.utils as fin\n",
    "import tqdm as tqdm\n",
    "from adaptive_latents import NumpyTimedDataSource\n",
    "import adaptive_latents.plotting_functions as bpf\n",
    "import adaptive_latents.input_sources.datasets as datasets\n",
    "import sklearn.decomposition\n",
    "import pandas as pd\n",
    "import adaptive_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define the Dataset to run on\n",
    "Each of the sub-sections (with the `<h2>`/`##` headings) of this part of the notebook imports a single dataset. If you run all of them in order only the last dataset (currently jPCA) will carry over to the next section, so you have to choose which cells you want to execute here.\n",
    "\n",
    "Each dataset-specific (jPCA/Buzaki/Indy/Fly/Musal) section constructs a `pre_datasets` variable, which is turned into a `datasets` dictionary in the \"For All\" cell. The `datasets` variable is how the rest of the notebook interacts with the data, and the `input_keys` and `output_keys` determine which variables get fed into bubblewrap and/or regressed.\n",
    "\n",
    "**To define a new set of variables**: Each key-value pair of the `pre_datasets` variable in every section is one possible set of variables to regress on or be a target of regression. The keys have a special format: \n",
    "```python\n",
    "key_string = f\"{human_readable_name} # {'i' if should_be_input} {'o' if should_be_regressed} {'ðŸŒ€' if should_add_shuffled_copy}\"\n",
    "```\n",
    "Note that the \"i\" \"o\" and \"ðŸŒ€\" tags can be added in any combination and any order (see the \"run this for all datasets\" cell). So this means that the key `'beh # i'` will be called \"beh\" in the labels of graphs and will only be used as an input to bubblewrap and not something that can be regressed. Note the \"ðŸŒ€\" tag is strictly additive; it basically means \"anything you were going to do with this dataset, also do that with a shuffled copy and keep the results for both\".\n",
    "\n",
    "The three functions I use the most on the right-hand-sides are `fin.prosvd_data` (which does proSVD), `fin.zscore` (which does a clipped running zscore), and `fin.clip` (which realigns datasets of different lengths). Note that due to the way I'm caching `prosvd_data`, it only takes keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chosen = \"indy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:41:29.649130Z",
     "start_time": "2023-11-03T14:41:28.410408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_chosen == \"fly\":\n",
    "    obs, raw_behavior, obs_t, beh_t = datasets.construct_fly_data(datasets.individual_identifiers[\"fly\"][0])\n",
    "    \n",
    "    resampled_behavior = fin.resample_matched_timeseries(raw_behavior, obs_t, beh_t)\n",
    "    beh = resampled_behavior\n",
    "    \n",
    "    pre_datasets = {\n",
    "        # 's(z(obs),1) # i': fin.prosvd_data(input_arr=fin.zscore(obs, 10), output_d=1, init_size=30),\n",
    "        # 's(z(obs),2) # i': fin.prosvd_data(input_arr=fin.zscore(obs, 10), output_d=2, init_size=30),\n",
    "        # 's(z(obs),3) # i': fin.prosvd_data(input_arr=fin.zscore(obs, 10), output_d=3, init_size=30),\n",
    "        's(z(obs),4) # i o': fin.prosvd_data(input_arr=fin.zscore(obs, 10), output_d=4, init_size=30),\n",
    "        # 's(z(obs),5) # i': fin.prosvd_data(input_arr=fin.zscore(obs, 10), output_d=5, init_size=30),\n",
    "        \n",
    "        's([z(obs),beh],4) # i': fin.prosvd_data(input_arr = np.hstack(fin.clip(fin.zscore(obs, 10), beh)), output_d=4, init_size=30),\n",
    "    \n",
    "        \n",
    "        'beh # o': beh,\n",
    "        'z(beh) # i': fin.zscore(input_arr=beh, init_size=20),\n",
    "    \n",
    "        # 's(obs,1) # o': fin.prosvd_data(input_arr=obs, output_d=1, init_size=30),\n",
    "        # 'z(beh) # i': fin.zscore(beh, 10),\n",
    "        # 's(z(beh),1) # o': fin.prosvd_data(input_arr=fin.zscore(beh, 10), output_d=1, init_size=20),\n",
    "        # 's(z([obs,beh]),5) # i': fin.prosvd_data(input_arr=fin.zscore(np.hstack([obs, beh])), output_d=5, init_size=20),\n",
    "        # 's(z([obs,beh]),1) # o': fin.prosvd_data(input_arr=fin.zscore(np.hstack([obs, beh])), output_d=1, init_size=20),\n",
    "    }\n",
    "    \n",
    "    for key, value in pre_datasets.items():\n",
    "        pre_datasets[key] = fin.clip(value, beh_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Buzsaki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_chosen == \"buzaki\":\n",
    "    obs, raw_behavior, bin_centers, beh_t = adaptive_latents.input_sources.datasets.construct_buzaki_data(individual_identifier=adaptive_latents.input_sources.datasets.individual_identifiers[\"buzaki\"][0], bin_width=0.03)\n",
    "    ###\n",
    "    resampled_behavior = fin.resample_matched_timeseries(raw_behavior, bin_centers, beh_t)\n",
    "    hd = np.arctan2(resampled_behavior[:,0] - resampled_behavior[:,2], resampled_behavior[:,1] - resampled_behavior[:,3])\n",
    "    beh = resampled_behavior[:,:2]\n",
    "    ###\n",
    "    pre_datasets = {\n",
    "        's([obs,pos],6) # i': fin.prosvd_data(input_arr=np.hstack([obs, beh]), output_d=6, init_size=50),\n",
    "        's(z([obs,pos]),4) # i': fin.prosvd_data(input_arr=fin.zscore(np.hstack([obs, beh])), output_d=4, init_size=50),\n",
    "        's(obs,6) # i o': fin.prosvd_data(input_arr=obs, output_d=6, init_size=50),\n",
    "        'pos # i o' : beh,\n",
    "        \n",
    "    \n",
    "        'hd # o': hd.reshape(-1,1),\n",
    "        # 'pca(obs,2) # i o' : sklearn.decomposition.PCA(n_components=2).fit_transform(obs),\n",
    "    }\n",
    "    \n",
    "    for key, value in pre_datasets.items():\n",
    "        pre_datasets[key] = fin.clip(value, beh_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Indy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_chosen == \"indy\":\n",
    "    obs, raw_behavior, bin_centers, beh_t = adaptive_latents.input_sources.datasets.construct_indy_data(\n",
    "        adaptive_latents.input_sources.datasets.individual_identifiers[\"indy\"][0],\n",
    "        bin_width=0.03,\n",
    "    )\n",
    "    \n",
    "    resampled_behavior = fin.resample_matched_timeseries(raw_behavior, bin_centers, beh_t)\n",
    "    beh = resampled_behavior\n",
    "    \n",
    "    pre_datasets = {\n",
    "        'beh # i o': beh,\n",
    "        's([obs,beh],6) # i': fin.prosvd_data(input_arr=np.hstack([obs, beh]), output_d=6, init_size=20),\n",
    "        's(obs,6) # i o': fin.prosvd_data(input_arr=obs, output_d=6, init_size=30),\n",
    "    \n",
    "    }\n",
    "    \n",
    "    for key, value in pre_datasets.items():\n",
    "        pre_datasets[key] = fin.clip(value, beh_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Musal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_chosen == \"musal\":\n",
    "    ca, vid, t_ca, t_vid = adaptive_latents.input_sources.datasets.generate_musal_dataset(cam=1, video_target_dim=6, resize_factor=1, prosvd_init_size=100)\n",
    "    \n",
    "    resampled_vid = fin.resample_matched_timeseries(vid, t_ca, t_vid)\n",
    "    \n",
    "    pre_datasets = {\n",
    "        's(ca,6) # i o': fin.prosvd_data(input_arr=ca, output_d=6, init_size=30),\n",
    "        's(vid,6) # i o': fin.prosvd_data(input_arr=resampled_vid, output_d=6, init_size=30),\n",
    "        's([ca,s(vid,6)],6) # i': fin.prosvd_data(input_arr=np.hstack([ca, resampled_vid]), output_d=6, init_size=30),\n",
    "    }\n",
    "    \n",
    "    for key, value in pre_datasets.items():\n",
    "        pre_datasets[key] = fin.clip(value, t_ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "tags": []
   },
   "source": [
    "## jPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_chosen == \"jPCA\":\n",
    "    obs, beh = fin.get_from_saved_npz(\"jpca_reduced_sc.npz\")\n",
    "    obs_t = beh_t = np.arange(beh.shape[0])\n",
    "    \n",
    "    pre_datasets = {\n",
    "        's(obs,4) # i o': fin.prosvd_data(input_arr=obs, output_d=4, init_size=30),\n",
    "        's([obs,beh],4) # i': fin.prosvd_data(input_arr=np.hstack([obs, beh]), output_d=4, init_size=30),\n",
    "        'beh # i o': beh,\n",
    "    }\n",
    "    \n",
    "    for key, value in pre_datasets.items():\n",
    "        pre_datasets[key] = fin.clip(value, beh_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Run Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run this for all datasets\n",
    "This constructs the `datasets` variable, which the rest of the script will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:41:44.511511Z",
     "start_time": "2023-11-03T14:41:44.501451Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with_randoms = {}\n",
    "for key, value in pre_datasets.items():\n",
    "    k = key.replace(\"ðŸŒ€\", \"\")\n",
    "    with_randoms[k] = value\n",
    "    if \"ðŸŒ€\" in key:\n",
    "        k, tags = k.split(\"#\")\n",
    "        k = k.strip()\n",
    "        with_randoms[f\"shuf({k}) #{tags.strip()}\"] = (*fin.shuffle_time(value[0]), value[1])\n",
    "\n",
    "datasets = {}\n",
    "input_keys = []\n",
    "output_keys = []\n",
    "for key, value in with_randoms.items():\n",
    "    k, tags = key.split(\"#\")\n",
    "    k = k.strip()\n",
    "    datasets[k] = value\n",
    "    assert np.all(np.isfinite(value[0]))\n",
    "    if \"i\" in tags:\n",
    "        input_keys.append(k)\n",
    "    if \"o\" in tags:\n",
    "        output_keys.append(k)\n",
    "        if \"b\" in tags:\n",
    "            a, t = value\n",
    "            for i in range(a.shape[1]):\n",
    "                new_k = k + f\"[:,{i}]\"\n",
    "                datasets[new_k] = (a[:,i:i+1],t)\n",
    "                output_keys.append(new_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Histograms\n",
    "The regressions tend to do better when the input and ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:42:08.250368Z",
     "start_time": "2023-11-03T14:42:07.651533Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# histograms for input datasets; if these are un-nice, bubblewrap will sometimes get nan's in the alphas and fail.\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(input_keys), nrows=1, figsize=(14,4), squeeze=False)\n",
    "\n",
    "for i, key in enumerate(input_keys):\n",
    "    ax[0][i].hist(datasets[key][0].flatten(), bins=100, density=True)\n",
    "    ax[0][i].set_title(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:42:10.383900Z",
     "start_time": "2023-11-03T14:42:10.051733Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# histograms for output datasets; note there will often be repeats\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(output_keys), figsize=(14,4), squeeze=False)\n",
    "for i, key in enumerate(output_keys):\n",
    "    ax[0][i].hist(datasets[key][0].flatten(), bins=100, density=True)\n",
    "    ax[0][i].set_title(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Run Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:42:14.658506Z",
     "start_time": "2023-11-03T14:42:14.651695Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@fin.save_to_cache(\"simple_bw_run\")\n",
    "def simple_bw_run(input_arr, t, time_offsets, bw_params):\n",
    "    bw = adaptive_latents.Bubblewrap(input_arr.shape[1], **bw_params)\n",
    "    br = adaptive_latents.BWRun(bw, adaptive_latents.NumpyTimedDataSource(input_arr, t, time_offsets), show_tqdm=True)\n",
    "    br.run(save=True)\n",
    "    return br\n",
    "\n",
    "def evaluate(i,o, maxlen=10_000, bw_params=adaptive_latents.default_parameters.default_jpca_dataset_parameters, seed=0):\n",
    "    \"\"\"\n",
    "    run bubblewrap on dataset i (the inputs) and run a regression on dataset o (the target outputs)\n",
    "    \n",
    "    Note that bubblewrap is run and then the regression is done post-hoc. This is so the bubblewrap run can be cached.\n",
    "    \n",
    "    parameters:\n",
    "        i: the input to bubblewrap; an element of the datasets dictionary (really tuple of `(datapoints, timestamps)`)\n",
    "        o: same as i, but the a set of variables to try to predict\n",
    "        maxlen: the point at which to cut off bubblewrap if the dataset is large\n",
    "        bw_params: the hyperparameters to pass to bubblewrap\n",
    "        \n",
    "    returns:\n",
    "        br: the bubblewrap run (this has lots of useful information)\n",
    "        pred: the predictions from the regression\n",
    "        true: the target values from the regression (so basically a slice of the `o` input)\n",
    "        times: the timestamps for the regression predictions\n",
    "    \"\"\"\n",
    "    i, i_t, o, o_t = fin.clip(*i, *o, maxlen=maxlen)\n",
    "    \n",
    "    o_dt = np.median(np.diff(o_t))\n",
    "    i_dt = np.median(np.diff(i_t))\n",
    "    n_steps = int(np.ceil(o_dt/i_dt))\n",
    "        \n",
    "    br = simple_bw_run(input_arr=i,t=i_t, time_offsets=[0,n_steps], bw_params=bw_params)\n",
    "    \n",
    "    \n",
    "    out_ds = adaptive_latents.NumpyTimedDataSource(o, o_t, (n_steps,))\n",
    "    reg_class = adaptive_latents.regressions.auto_regression_decorator(adaptive_latents.regressions.SymmetricNoisyRegressor, n_steps=0, autoregress_only=True)\n",
    "    reg = reg_class(input_d=br.bw.N, output_d=o.shape[1])\n",
    "    \n",
    "    br.add_regression_post_hoc(reg, out_ds)\n",
    "    pred = br.h.beh_pred[n_steps]\n",
    "    true = br.h.beh_pred[n_steps] - br.h.beh_error[n_steps]\n",
    "    times = br.h.reg_offset_t[n_steps]\n",
    "    return br, pred, true, times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# this cell evaluates each element of the table specified above using the `evaluate` function\n",
    "\n",
    "bw_params = dict(    \n",
    "    adaptive_latents.default_parameters.default_jpca_dataset_parameters, \n",
    "num=100,\n",
    "eps=1e-3,\n",
    "step=1,\n",
    "num_grad_q=3,\n",
    ")\n",
    "\n",
    "results = {}\n",
    "brs = {}\n",
    "true_values = {}\n",
    "for okey in output_keys:\n",
    "    results[okey] = {}\n",
    "    brs[okey] = {}\n",
    "    true_values[okey] = {}\n",
    "    for ikey in input_keys:\n",
    "        print(f\"{okey= } {ikey= }\")\n",
    "\n",
    "        br, pred, true, times = evaluate(datasets[ikey], datasets[okey], maxlen=5_000, bw_params=bw_params)\n",
    "\n",
    "        results[okey][ikey] = pred\n",
    "        brs[okey][ikey] = br\n",
    "        true_values[okey][ikey] = (true, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell plots the table of results\n",
    "\n",
    "def get_corrs_from_br(br):\n",
    "    offset = br.output_ds.time_offsets[-1]\n",
    "    \n",
    "    first_pred_idx = np.nonzero(~np.all(np.isnan(br.h.beh_error[1]),axis=1))[0][0]\n",
    "    n_samples_before_end = (br.h.beh_error[1].shape[0] - first_pred_idx)//2\n",
    "    \n",
    "    lh_predicted = br.h.beh_pred[offset][-n_samples_before_end:]\n",
    "    lh_true = br.h.beh_pred[offset][-n_samples_before_end:] - br.h.beh_error[offset][-n_samples_before_end:]\n",
    "    \n",
    "    corrs = [np.corrcoef(lh_true[:,j], lh_predicted[:,j])[0,1] for j in range(lh_true.shape[1])]\n",
    "    return corrs\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(output_keys), ncols=len(input_keys), figsize=(14,2*len(output_keys)), layout='tight', squeeze=False)\n",
    "\n",
    "s = slice(-1)\n",
    "\n",
    "for row, okey in enumerate(output_keys):\n",
    "    ax[row,0].set_ylabel(\"â†’\" + okey)\n",
    "    for col, ikey in enumerate(input_keys):\n",
    "        true, times, predicted = fin.clip(*true_values[okey][ikey], results[okey][ikey])\n",
    "        \n",
    "        br = brs[okey][ikey]\n",
    "\n",
    "        corrs = get_corrs_from_br(br)\n",
    "\n",
    "        ax[row,col].plot(true[s])\n",
    "        ax[row,col].plot(predicted[s])\n",
    "        \n",
    "        if true.shape[1] < 4:\n",
    "            corrs = \" \".join([f\"{c:.2f}\" for c in corrs])\n",
    "        else:\n",
    "            corrs = f\"{np.mean(corrs):.3f}\"\n",
    "        ax[row,col].text(.01,.99, corrs, ha='left', va='top', transform=ax[row, col].transAxes)\n",
    "        if row == 0:\n",
    "            ax[0,col].set_title(ikey +  \"â†’\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell plots the table of results\n",
    "\n",
    "def get_corrs_from_br(br):\n",
    "    offset = br.output_ds.time_offsets[-1]\n",
    "    \n",
    "    first_pred_idx = np.nonzero(~np.all(np.isnan(br.h.beh_error[1]),axis=1))[0][0]\n",
    "    n_samples_before_end = (br.h.beh_error[1].shape[0] - first_pred_idx)//2\n",
    "    \n",
    "    lh_predicted = br.h.beh_pred[offset][-n_samples_before_end:]\n",
    "    lh_true = br.h.beh_pred[offset][-n_samples_before_end:] - br.h.beh_error[offset][-n_samples_before_end:]\n",
    "    \n",
    "    corrs = [np.corrcoef(lh_true[:,j], lh_predicted[:,j])[0,1] for j in range(lh_true.shape[1])]\n",
    "    return corrs\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(output_keys), ncols=len(input_keys), figsize=(14,2*len(output_keys)), layout='tight', squeeze=False)\n",
    "\n",
    "s = slice(-1)\n",
    "\n",
    "for row, okey in enumerate(output_keys):\n",
    "    ax[row,0].set_ylabel(\"â†’\" + okey)\n",
    "    for col, ikey in enumerate(input_keys):\n",
    "        true, times, predicted = fin.clip(*true_values[okey][ikey], results[okey][ikey])\n",
    "        \n",
    "        br = brs[okey][ikey]\n",
    "\n",
    "        corrs = get_corrs_from_br(br)\n",
    "\n",
    "        ax[row,col].plot(true[s])\n",
    "        ax[row,col].plot(predicted[s])\n",
    "        \n",
    "        if true.shape[1] < 4:\n",
    "            corrs = \" \".join([f\"{c:.2f}\" for c in corrs])\n",
    "        else:\n",
    "            corrs = f\"{np.mean(corrs):.3f}\"\n",
    "        ax[row,col].text(.01,.99, corrs, ha='left', va='top', transform=ax[row, col].transAxes)\n",
    "        if row == 0:\n",
    "            ax[0,col].set_title(ikey +  \"â†’\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell plots the table of results\n",
    "\n",
    "def get_corrs_from_br(br):\n",
    "    offset = br.output_ds.time_offsets[-1]\n",
    "    \n",
    "    first_pred_idx = np.nonzero(~np.all(np.isnan(br.h.beh_error[1]),axis=1))[0][0]\n",
    "    n_samples_before_end = (br.h.beh_error[1].shape[0] - first_pred_idx)//2\n",
    "    \n",
    "    lh_predicted = br.h.beh_pred[offset][-n_samples_before_end:]\n",
    "    lh_true = br.h.beh_pred[offset][-n_samples_before_end:] - br.h.beh_error[offset][-n_samples_before_end:]\n",
    "    \n",
    "    corrs = [np.corrcoef(lh_true[:,j], lh_predicted[:,j])[0,1] for j in range(lh_true.shape[1])]\n",
    "    return corrs\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(output_keys), ncols=len(input_keys), figsize=(14,2*len(output_keys)), layout='tight', squeeze=False)\n",
    "\n",
    "s = slice(-1)\n",
    "\n",
    "for row, okey in enumerate(output_keys):\n",
    "    ax[row,0].set_ylabel(\"â†’\" + okey)\n",
    "    for col, ikey in enumerate(input_keys):\n",
    "        true, times, predicted = fin.clip(*true_values[okey][ikey], results[okey][ikey])\n",
    "        \n",
    "        br = brs[okey][ikey]\n",
    "\n",
    "        corrs = get_corrs_from_br(br)\n",
    "\n",
    "        ax[row,col].plot(true[s])\n",
    "        ax[row,col].plot(predicted[s])\n",
    "        \n",
    "        if true.shape[1] < 4:\n",
    "            corrs = \" \".join([f\"{c:.2f}\" for c in corrs])\n",
    "        else:\n",
    "            corrs = f\"{np.mean(corrs):.3f}\"\n",
    "        ax[row,col].text(.01,.99, corrs, ha='left', va='top', transform=ax[row, col].transAxes)\n",
    "        if row == 0:\n",
    "            ax[0,col].set_title(ikey +  \"â†’\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell plots the table of results\n",
    "\n",
    "def get_corrs_from_br(br):\n",
    "    offset = br.output_ds.time_offsets[-1]\n",
    "    \n",
    "    first_pred_idx = np.nonzero(~np.all(np.isnan(br.h.beh_error[1]),axis=1))[0][0]\n",
    "    n_samples_before_end = (br.h.beh_error[1].shape[0] - first_pred_idx)//2\n",
    "    \n",
    "    lh_predicted = br.h.beh_pred[offset][-n_samples_before_end:]\n",
    "    lh_true = br.h.beh_pred[offset][-n_samples_before_end:] - br.h.beh_error[offset][-n_samples_before_end:]\n",
    "    \n",
    "    corrs = [np.corrcoef(lh_true[:,j], lh_predicted[:,j])[0,1] for j in range(lh_true.shape[1])]\n",
    "    return corrs\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(output_keys), ncols=len(input_keys), figsize=(14,2*len(output_keys)), layout='tight', squeeze=False)\n",
    "\n",
    "s = slice(-1)\n",
    "\n",
    "for row, okey in enumerate(output_keys):\n",
    "    ax[row,0].set_ylabel(\"â†’\" + okey)\n",
    "    for col, ikey in enumerate(input_keys):\n",
    "        true, times, predicted = fin.clip(*true_values[okey][ikey], results[okey][ikey])\n",
    "        \n",
    "        br = brs[okey][ikey]\n",
    "\n",
    "        corrs = get_corrs_from_br(br)\n",
    "\n",
    "        ax[row,col].plot(true[s])\n",
    "        ax[row,col].plot(predicted[s])\n",
    "        \n",
    "        if true.shape[1] < 4:\n",
    "            corrs = \" \".join([f\"{c:.2f}\" for c in corrs])\n",
    "        else:\n",
    "            corrs = f\"{np.mean(corrs):.3f}\"\n",
    "        ax[row,col].text(.01,.99, corrs, ha='left', va='top', transform=ax[row, col].transAxes)\n",
    "        if row == 0:\n",
    "            ax[0,col].set_title(ikey +  \"â†’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table summary for Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_to_break_out = \"s(obs,4)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(brs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(br):\n",
    "    lh_pred, lh_true, _ = br.get_behavior_last_half(br.obs_ds.time_offsets[-1])\n",
    "    l = [np.corrcoef(np.squeeze([lh_pred[:,j], lh_true[:,j]]))[0,1] for j in range(lh_true.shape[1])]\n",
    "    if len(l) < 4:\n",
    "        pass\n",
    "    else:\n",
    "        l = [np.mean(l)]\n",
    "    return \", \".join([str(round(x,2)) for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table = df.applymap(f)\n",
    "table.loc[\"pred\"] = df.applymap(lambda x: x.log_pred_p_summary(offset=1)).mean(axis=0)\n",
    "table.loc[\"ent\"] =  df.applymap(lambda x: x.entropy_summary(offset=1)).mean(axis=0)\n",
    "# the mean in the above two lines is just a formality; technically the values might be different across rows because of clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = {key: get_corrs_from_br(value) for key, value in brs[row_to_break_out].items()}\n",
    "temp = pd.DataFrame(temp, index=[row_to_break_out + f\"_{n}\" for n in range(datasets[row_to_break_out][0].shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.concat([table, temp]) # you can copy-paste this right into Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(temp.shape[0]):\n",
    "    jitter = (rng.random(size=temp.shape[1])-.5) * .1\n",
    "    plt.plot(np.arange(temp.shape[1]) + jitter, temp.iloc[i,:], 'k.')\n",
    "    plt.plot(np.arange(temp.shape[1]) + jitter, temp.iloc[i,:], 'k-', alpha=.5)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(ticks = np.arange(temp.shape[1]), labels=temp.columns)\n",
    "plt.ylabel(f'individual r^2 per component in \"{row_to_break_out}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Optimizing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets.keys() # this is so I have the text of the keys ready to copy-paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brs2 = [] # this is a history of bubblewrap runs; reset it when the plots are getting too full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "brs2.append(evaluate(i=datasets['s(obs,4)'], o=datasets['s(obs,4)'], maxlen=5_000, \n",
    "                     bw_params=\n",
    "                     dict(\n",
    "                        adaptive_latents.default_parameters.default_jpca_dataset_parameters,\n",
    "                        num=100,\n",
    "                        eps=1e-5,\n",
    "                        step=.5,\n",
    "                        num_grad_q=4,\n",
    "                     )\n",
    "                    )[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(brs2) < 8 # this is actually fine, but just be aware that the later part of the cell slices \n",
    "\n",
    "colors = ['k']*(len(brs2))\n",
    "colors[-1] = 'r'\n",
    "bpf.compare_metrics(brs2[-7:], colors=colors[-7:], offset=1, show_legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Choosing the better predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_key = output_keys[0]\n",
    "output_key = \"s(obs,6)[:,5]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_steps_ahead = 1\n",
    "\n",
    "br = brs[output_key][input_keys[0]]\n",
    "truth = br.behavior_pred_history[n_steps_ahead] - br.behavior_error_history[n_steps_ahead]\n",
    "\n",
    "clipped = fin.clip(\n",
    "    truth,\n",
    "    *[brs[output_key][input_key].behavior_pred_history[n_steps_ahead] for input_key in brs[output_key]]\n",
    ")\n",
    "truth = clipped[0]\n",
    "preds = np.squeeze(clipped[1:]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_estimate = []\n",
    "errors = preds - truth\n",
    "\n",
    "n_to_consider = 4\n",
    "\n",
    "for i in range(1,len(errors)):\n",
    "    a = errors[i-n_to_consider:i]\n",
    "    bests = (np.abs(a) - np.abs(a).min(axis=1)[:,None]) == 0\n",
    "    best_last_time = np.argmax(bests.sum(axis=0))\n",
    "    combined_estimate.append(preds[i, best_last_time])\n",
    "combined_estimate = np.squeeze(combined_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds, combined_estimate, truth = fin.clip(preds, combined_estimate, truth)\n",
    "s = slice(1500, 1600)\n",
    "plt.plot(truth[s], 'k');\n",
    "plt.plot(preds[s], 'r', alpha=0.25)\n",
    "plt.plot(combined_estimate[s], 'r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heights = list((errors**2).mean(axis=0)) + [(combined_estimate**2).mean()]\n",
    "x = list(np.arange(errors.shape[1] + 1))\n",
    "plt.bar(x, heights)\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"predictor\")\n",
    "plt.xticks(x, labels=[xx for xx in x[:-1]] + [\"combined\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.corrcoef(truth.T, combined_estimate[:,None].T)[0,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
