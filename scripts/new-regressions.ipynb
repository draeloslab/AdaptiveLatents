{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:41:21.104905Z",
     "start_time": "2023-11-03T14:41:20.201483Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import adaptive_latents.transforms.utils as fin\n",
    "import tqdm as tqdm\n",
    "from adaptive_latents import NumpyTimedDataSource\n",
    "import adaptive_latents.plotting_functions as bpf\n",
    "import adaptive_latents.input_sources.datasets as datasets\n",
    "import sklearn.decomposition\n",
    "import pandas as pd\n",
    "import adaptive_latents\n",
    "from adaptive_latents.transforms.jpca import apply_prosvd_and_sjpca_and_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_columns(X, t, kernel_length=5, kernel_end=-3):\n",
    "    kernel = np.exp(np.linspace(0, kernel_end, kernel_length))\n",
    "    kernel /= kernel.sum()\n",
    "    mode = 'valid'\n",
    "    X = np.column_stack([np.convolve(kernel, column, mode) for column in X.T])\n",
    "    t = np.convolve(np.hstack([[1],kernel[:-1]*0]), t, mode)\n",
    "    return X, t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chosen = \"jenkins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_chosen == \"buzaki\":\n",
    "    obs, raw_behavior, bin_centers, beh_t = adaptive_latents.input_sources.datasets.construct_buzaki_data(individual_identifier=adaptive_latents.input_sources.datasets.individual_identifiers[\"buzaki\"][0], bin_width=0.03)\n",
    "    ###\n",
    "    resampled_behavior = fin.resample_matched_timeseries(raw_behavior, bin_centers, beh_t)\n",
    "    hd = np.arctan2(resampled_behavior[:,0] - resampled_behavior[:,2], resampled_behavior[:,1] - resampled_behavior[:,3])\n",
    "    beh = resampled_behavior[:,:2]\n",
    "    ###\n",
    "    pre_datasets = {\n",
    "        's([obs,pos],6) # i': fin.prosvd_data(input_arr=np.hstack([obs, beh]), output_d=6, init_size=50),\n",
    "        's(z([obs,pos]),4) # i': fin.prosvd_data(input_arr=fin.zscore(np.hstack([obs, beh])), output_d=4, init_size=50),\n",
    "        's(obs,6) # i o': fin.prosvd_data(input_arr=obs, output_d=6, init_size=50),\n",
    "        'pos # i o' : beh,\n",
    "        \n",
    "    \n",
    "        'hd # o': hd.reshape(-1,1),\n",
    "        # 'pca(obs,2) # i o' : sklearn.decomposition.PCA(n_components=2).fit_transform(obs),\n",
    "    }\n",
    "    \n",
    "    for key, value in pre_datasets.items():\n",
    "        pre_datasets[key] = fin.clip(value, beh_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Indy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if dataset_chosen == \"indy\":\n",
    "\n",
    "    \n",
    "    obs, beh, bin_centers, beh_t = adaptive_latents.input_sources.datasets.construct_indy_data(\n",
    "        adaptive_latents.input_sources.datasets.individual_identifiers[\"indy\"][0],\n",
    "        bin_width=0.03,\n",
    "    )\n",
    "    \n",
    "    resampled_behavior = fin.resample_matched_timeseries(beh, bin_centers, beh_t)\n",
    "    beh, beh_t = resampled_behavior, bin_centers\n",
    "\n",
    "\n",
    "    pre_smooth_obs, pre_smooth_bin_centers = obs, bin_centers\n",
    "    obs, bin_centers = smooth_columns(obs, bin_centers, kernel_end=-3)\n",
    "\n",
    "    \n",
    "    \n",
    "    pre_datasets = {\n",
    "        'beh # o': (beh, beh_t),\n",
    "        'j(s(obs,4))[:,0] # i o': fin.clip(apply_prosvd_and_sjpca_and_cache(input_arr=obs, intermediate_d=4, output_d=4)[:,0], bin_centers),\n",
    "        's(obs,4) # i o': fin.clip(fin.prosvd_data(input_arr=obs, output_d=4, init_size=30, centering=True), bin_centers),\n",
    "        'j(s(obs,4)) # i o': fin.clip(apply_prosvd_and_sjpca_and_cache(input_arr=obs, intermediate_d=4, output_d=4), bin_centers),\n",
    "\n",
    "    \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Jenkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_chosen == \"jenkins\":\n",
    "    obs, beh, obs_t, beh_t = adaptive_latents.input_sources.datasets.construct_jenkins_data(bin_width=0.06)\n",
    "    beh, beh_t = fin.resample_matched_timeseries(beh, obs_t, beh_t), obs_t\n",
    "    obs, obs_t = smooth_columns(obs, obs_t, kernel_length=20)\n",
    "    pre_datasets = {\n",
    "        's(obs,4) # i o': (fin.prosvd_data(input_arr=obs, output_d=4, init_size=4, centering=True), obs_t),\n",
    "        'j(s(obs,4)) # i o': (apply_prosvd_and_sjpca_and_cache(input_arr=obs, intermediate_d=4, output_d=4), obs_t),\n",
    "        'j(s(obs,4))[:,0] # o': (apply_prosvd_and_sjpca_and_cache(input_arr=obs, intermediate_d=4, output_d=4)[:,0:1], obs_t),\n",
    "        's(obs,4)[:,0] # o': (fin.prosvd_data(input_arr=obs, output_d=4, init_size=4, centering=True)[:,0:1], obs_t),\n",
    "        'beh # o': (beh, beh_t),\n",
    "    }\n",
    "    \n",
    "    for key, value in pre_datasets.items():\n",
    "        x, x_t = fin.clip(*value)\n",
    "        idx = ~np.any(np.isnan(x), axis=1)\n",
    "        x, x_t = x[idx], x_t[idx]\n",
    "        pre_datasets[key] = (x, x_t)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:41:44.511511Z",
     "start_time": "2023-11-03T14:41:44.501451Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with_randoms = {}\n",
    "for key, value in pre_datasets.items():\n",
    "    k = key.replace(\"ğŸŒ€\", \"\")\n",
    "    with_randoms[k] = value\n",
    "    if \"ğŸŒ€\" in key:\n",
    "        k, tags = k.split(\"#\")\n",
    "        k = k.strip()\n",
    "        with_randoms[f\"shuf({k}) #{tags.strip()}\"] = (*fin.shuffle_time(value[0]), value[1])\n",
    "\n",
    "datasets = {}\n",
    "input_keys = []\n",
    "output_keys = []\n",
    "for key, value in with_randoms.items():\n",
    "    k, tags = key.split(\"#\")\n",
    "    k = k.strip()\n",
    "    datasets[k] = value\n",
    "    assert np.all(np.isfinite(value[0]))\n",
    "    if \"i\" in tags:\n",
    "        input_keys.append(k)\n",
    "    if \"o\" in tags:\n",
    "        output_keys.append(k)\n",
    "        if \"b\" in tags:\n",
    "            a, t = value\n",
    "            for i in range(a.shape[1]):\n",
    "                new_k = k + f\"[:,{i}]\"\n",
    "                datasets[new_k] = (a[:,i:i+1],t)\n",
    "                output_keys.append(new_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:42:08.250368Z",
     "start_time": "2023-11-03T14:42:07.651533Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# histograms for input datasets; if these are un-nice, bubblewrap will sometimes get nan's in the alphas and fail.\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(ncols=len(input_keys)-1, nrows=1, figsize=(9,4), squeeze=False)\n",
    "\n",
    "\n",
    "t2 = t1 + 5\n",
    "for i, key in enumerate(input_keys[:-1]):\n",
    "    if len(datasets[key][0].shape) > 1 and datasets[key][0].shape[1] > 1:\n",
    "        x, x_t = datasets[key]\n",
    "        s = (t1 < x_t) & (x_t < t2)\n",
    "        # ax[0,i].scatter(x[:,0], x[:,1], s=10)\n",
    "        ax[0,i].hist2d(x[:,0], x[:,1], [50,50])\n",
    "        ax[0,i].plot(x[s,0], x[s,1], '-', color='k', linewidth=2)\n",
    "        ax[0,i].plot(x[s,0], x[s,1], '-', color='w', linewidth=1.5)\n",
    "        ax[0,i].axis('off')\n",
    "    # ax[0][i].set_title(key)\n",
    "fig.savefig(fname=\"/home/jgould//Downloads/trajectories.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Run Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:42:14.658506Z",
     "start_time": "2023-11-03T14:42:14.651695Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _simple_bw_run(input_arr, t, time_offsets, bw_params, n_steps):\n",
    "    bw = adaptive_latents.Bubblewrap(input_arr.shape[1], **bw_params)\n",
    "    br = adaptive_latents.BWRun(bw, adaptive_latents.NumpyTimedDataSource(input_arr, t, time_offsets), show_tqdm=True)\n",
    "    br.run(limit=n_steps,save=False)\n",
    "    return br\n",
    "simple_bw_run = fin.save_to_cache(\"simple_bw_run\")(_simple_bw_run)\n",
    "\n",
    "def evaluate(i,o, maxlen=10_000, bw_params=adaptive_latents.default_parameters.default_jpca_dataset_parameters, seed=0, cache=True):\n",
    "    \"\"\"\n",
    "    run bubblewrap on dataset i (the inputs) and run a regression on dataset o (the target outputs)\n",
    "    \n",
    "    Note that bubblewrap is run and then the regression is done post-hoc. This is so the bubblewrap run can be cached.\n",
    "    \n",
    "    parameters:\n",
    "        i: the input to bubblewrap; an element of the datasets dictionary (really tuple of `(datapoints, timestamps)`)\n",
    "        o: same as i, but the a set of variables to try to predict\n",
    "        maxlen: the point at which to cut off bubblewrap if the dataset is large\n",
    "        bw_params: the hyperparameters to pass to bubblewrap\n",
    "        \n",
    "    returns:\n",
    "        br: the bubblewrap run (this has lots of useful information)\n",
    "        pred: the predictions from the regression\n",
    "        true: the target values from the regression (so basically a slice of the `o` input)\n",
    "        times: the timestamps for the regression predictions\n",
    "    \"\"\"\n",
    "    i, i_t = fin.clip(*i)\n",
    "    o, o_t = fin.clip(*o)\n",
    "    \n",
    "    o_dt = np.median(np.diff(o_t))\n",
    "    i_dt = np.median(np.diff(i_t))\n",
    "    assert int(np.ceil(o_dt/i_dt)) == 1\n",
    "    n_steps = 1\n",
    "\n",
    "    if cache:\n",
    "        br = simple_bw_run(input_arr=i, t=i_t, time_offsets=[0,n_steps], bw_params=bw_params, n_steps=maxlen)\n",
    "    else:\n",
    "        br = _simple_bw_run(input_arr=i, t=i_t, time_offsets=[0,n_steps], bw_params=bw_params, n_steps=maxlen)\n",
    "    \n",
    "    \n",
    "    out_ds = adaptive_latents.NumpyTimedDataSource(o, o_t, (n_steps,))\n",
    "    reg_class = adaptive_latents.regressions.auto_regression_decorator(adaptive_latents.regressions.SemiRegularizedRegressor, n_steps=0, autoregress_only=False)\n",
    "    reg = reg_class(input_d=br.bw.N, output_d=o.shape[1], regularization_factor=.001)\n",
    "    \n",
    "    br.add_regression_post_hoc(reg, out_ds)\n",
    "    pred = br.h.beh_pred[n_steps]\n",
    "    true = br.h.beh_pred[n_steps] - br.h.beh_error[n_steps]\n",
    "    times = br.h.reg_offset_t[n_steps]\n",
    "    return br, pred, true, times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# this cell evaluates each element of the table specified above using the `evaluate` function\n",
    "\n",
    "bw_params = dict(    \n",
    "    adaptive_latents.default_parameters.default_jpca_dataset_parameters, \n",
    "num=100,\n",
    "eps=1e-3,\n",
    "step=8e-1,\n",
    "num_grad_q=2,\n",
    ")\n",
    "\n",
    "results = {}\n",
    "brs = {}\n",
    "true_values = {}\n",
    "for okey in output_keys:\n",
    "    results[okey] = {}\n",
    "    brs[okey] = {}\n",
    "    true_values[okey] = {}\n",
    "    for ikey in input_keys:\n",
    "        print(f\"{okey= } {ikey= }\")\n",
    "\n",
    "        br, pred, true, times = evaluate(datasets[ikey], datasets[okey], maxlen=3_000, bw_params=bw_params)\n",
    "\n",
    "        results[okey][ikey] = pred\n",
    "        brs[okey][ikey] = br\n",
    "        true_values[okey][ikey] = (true, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "adaptive_latents.plotting_functions.compare_metrics([brs['beh']['j(s(obs,4))'] ], 1)\n",
    "print(brs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell plots the table of results\n",
    "%matplotlib inline\n",
    "def get_corrs_from_br(br):\n",
    "    offset = br.output_ds.time_offsets[-1]\n",
    "    \n",
    "    first_pred_idx = np.nonzero(~np.all(np.isnan(br.h.beh_error[offset]),axis=1))[0][0]\n",
    "    n_samples_before_end = (br.h.beh_error[offset].shape[0] - first_pred_idx)//2\n",
    "    \n",
    "    lh_predicted = br.h.beh_pred[offset][-n_samples_before_end:]\n",
    "    lh_true = br.h.beh_pred[offset][-n_samples_before_end:] - br.h.beh_error[offset][-n_samples_before_end:]\n",
    "    \n",
    "    corrs = [np.corrcoef(lh_true[:,j], lh_predicted[:,j])[0,1] for j in range(lh_true.shape[1])]\n",
    "    sq_errors = ((lh_predicted - lh_true)**2).mean(axis=0)\n",
    "    return corrs\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(output_keys), ncols=len(input_keys), figsize=(14,2*len(output_keys)), layout='tight', squeeze=False)\n",
    "\n",
    "s = slice(-1)\n",
    "\n",
    "for row, okey in enumerate(output_keys):\n",
    "    ax[row,0].set_ylabel(\"â†’\" + okey)\n",
    "    for col, ikey in enumerate(input_keys):\n",
    "        true, times, predicted = fin.clip(*true_values[okey][ikey], results[okey][ikey])\n",
    "        \n",
    "        br = brs[okey][ikey]\n",
    "\n",
    "        corrs = get_corrs_from_br(br)\n",
    "\n",
    "        ax[row,col].plot(true[s])\n",
    "        ax[row,col].plot(predicted[s], '-')\n",
    "        \n",
    "        if true.shape[1] < 4:\n",
    "            corrs = \" \".join([f\"{c:.4f}\" for c in corrs])\n",
    "        else:\n",
    "            corrs = f\"{np.mean(corrs):.3f}\"\n",
    "        ax[row,col].text(.01,.99, corrs, ha='left', va='top', transform=ax[row, col].transAxes)\n",
    "        if row == 0:\n",
    "            ax[0,col].set_title(ikey +  \"â†’\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(12,5), layout='tight', squeeze=False, sharex=True)\n",
    "\n",
    "for row, okey in enumerate(['beh', 'j(s(obs,4))[:,0]', 's(obs,4)[:,0]']):\n",
    "    # ax[row,0].set_ylabel(\"â†’\" + okey)\n",
    "    for col, ikey in enumerate(['s(obs,4)', 'j(s(obs,4))',]):\n",
    "        true, times, predicted = fin.clip(*true_values[okey][ikey], results[okey][ikey])\n",
    "        \n",
    "        br = brs[okey][ikey]\n",
    "\n",
    "        corrs = get_corrs_from_br(br)\n",
    "\n",
    "        \n",
    "        ax[row,col].plot(true[s], 'k')\n",
    "        ax[row,col].plot(predicted[s], '-')\n",
    "\n",
    "        ax[row,col].set_xlim([1900, 2900])\n",
    "\n",
    "        if true.shape[1] < 4:\n",
    "            corrs = \" \".join([f\"{c:.4f}\" for c in corrs])\n",
    "        else:\n",
    "            corrs = f\"{np.mean(corrs):.3f}\"\n",
    "        ax[row,col].text(.01,.99, corrs, ha='left', va='top', transform=ax[row, col].transAxes)\n",
    "        \n",
    "        if row == 0:\n",
    "            ax[0,col].set_title(ikey +  \" â†’\")\n",
    "for a in ax[-1,:]:\n",
    "    a.set_xlabel(\"time (s)\")\n",
    "ax[0,0].set_ylabel(\"behavior (mm)\")\n",
    "ax[1,0].set_ylabel(\"jpca latent 1\")\n",
    "ax[2,0].set_ylabel(\"proSVD latent 1\")\n",
    "\n",
    "fig.savefig(fname=\"/home/jgould//Downloads/predictions.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell plots the table of results\n",
    "\n",
    "def get_corrs_from_br(br):\n",
    "    offset = br.output_ds.time_offsets[-1]\n",
    "    \n",
    "    first_pred_idx = np.nonzero(~np.all(np.isnan(br.h.beh_error[offset]),axis=1))[0][0]\n",
    "    n_samples_before_end = (br.h.beh_error[offset].shape[0] - first_pred_idx)//2\n",
    "    \n",
    "    lh_predicted = br.h.beh_pred[offset][-n_samples_before_end:]\n",
    "    lh_true = br.h.beh_pred[offset][-n_samples_before_end:] - br.h.beh_error[offset][-n_samples_before_end:]\n",
    "    \n",
    "    corrs = [np.corrcoef(lh_true[:,j], lh_predicted[:,j])[0,1] for j in range(lh_true.shape[1])]\n",
    "    sq_errors = ((lh_predicted - lh_true)**2).mean(axis=0)\n",
    "    return corrs\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(output_keys), ncols=len(input_keys), figsize=(14,2*len(output_keys)), layout='tight', squeeze=False)\n",
    "\n",
    "s = slice(-1)\n",
    "\n",
    "for row, okey in enumerate(output_keys):\n",
    "    ax[row,0].set_ylabel(\"â†’\" + okey)\n",
    "    for col, ikey in enumerate(input_keys):\n",
    "        true, times, predicted = fin.clip(*true_values[okey][ikey], results[okey][ikey])\n",
    "        \n",
    "        br = brs[okey][ikey]\n",
    "\n",
    "        corrs = get_corrs_from_br(br)\n",
    "\n",
    "        ax[row,col].plot(true[s])\n",
    "        ax[row,col].plot(predicted[s])\n",
    "        \n",
    "        if true.shape[1] < 4:\n",
    "            corrs = \" \".join([f\"{c:.4f}\" for c in corrs])\n",
    "        else:\n",
    "            corrs = f\"{np.mean(corrs):.3f}\"\n",
    "        ax[row,col].text(.01,.99, corrs, ha='left', va='top', transform=ax[row, col].transAxes)\n",
    "        if row == 0:\n",
    "            ax[0,col].set_title(ikey +  \"â†’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Table summary for Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_to_break_out = \"s(obs,4)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(brs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(br):\n",
    "    lh_pred, lh_true, _ = br.get_behavior_last_half(br.obs_ds.time_offsets[-1])\n",
    "    l = [np.corrcoef(np.squeeze([lh_pred[:,j], lh_true[:,j]]))[0,1] for j in range(lh_true.shape[1])]\n",
    "    if len(l) < 4:\n",
    "        pass\n",
    "    else:\n",
    "        l = [np.mean(l)]\n",
    "    return \", \".join([str(round(x,2)) for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table = df.applymap(f)\n",
    "table.loc[\"pred\"] = df.applymap(lambda x: x.log_pred_p_summary(offset=1)).mean(axis=0)\n",
    "table.loc[\"ent\"] =  df.applymap(lambda x: x.entropy_summary(offset=1)).mean(axis=0)\n",
    "# the mean in the above two lines is just a formality; technically the values might be different across rows because of clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = {key: get_corrs_from_br(value) for key, value in brs[row_to_break_out].items()}\n",
    "temp = pd.DataFrame(temp, index=[row_to_break_out + f\"_{n}\" for n in range(datasets[row_to_break_out][0].shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.concat([table, temp]) # you can copy-paste this right into Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(temp.shape[0]):\n",
    "    jitter = (rng.random(size=temp.shape[1])-.5) * .1\n",
    "    plt.plot(np.arange(temp.shape[1]) + jitter, temp.iloc[i,:], 'k.')\n",
    "    plt.plot(np.arange(temp.shape[1]) + jitter, temp.iloc[i,:], 'k-', alpha=.5)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(ticks = np.arange(temp.shape[1]), labels=temp.columns)\n",
    "plt.ylabel(f'individual r^2 per component in \"{row_to_break_out}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Optimizing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets.keys() # this is so I have the text of the keys ready to copy-paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brs2 = [] # this is a history of bubblewrap runs; reset it when the plots are getting too full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "brs2.append(evaluate(i=datasets['s(obs,4)'], o=datasets['s(obs,4)'], maxlen=5_000, \n",
    "                     bw_params=\n",
    "                     dict(\n",
    "                        adaptive_latents.default_parameters.default_jpca_dataset_parameters,\n",
    "                        num=100,\n",
    "                        eps=1e-5,\n",
    "                        step=.5,\n",
    "                        num_grad_q=4,\n",
    "                     )\n",
    "                    )[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(brs2) < 8 # this is actually fine, but just be aware that the later part of the cell slices \n",
    "\n",
    "colors = ['k']*(len(brs2))\n",
    "colors[-1] = 'r'\n",
    "bpf.compare_metrics(brs2[-7:], colors=colors[-7:], offset=1, show_legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Choosing the better predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_key = output_keys[0]\n",
    "output_key = \"s(obs,6)[:,5]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_steps_ahead = 1\n",
    "\n",
    "br = brs[output_key][input_keys[0]]\n",
    "truth = br.behavior_pred_history[n_steps_ahead] - br.behavior_error_history[n_steps_ahead]\n",
    "\n",
    "clipped = fin.clip(\n",
    "    truth,\n",
    "    *[brs[output_key][input_key].behavior_pred_history[n_steps_ahead] for input_key in brs[output_key]]\n",
    ")\n",
    "truth = clipped[0]\n",
    "preds = np.squeeze(clipped[1:]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_estimate = []\n",
    "errors = preds - truth\n",
    "\n",
    "n_to_consider = 4\n",
    "\n",
    "for i in range(1,len(errors)):\n",
    "    a = errors[i-n_to_consider:i]\n",
    "    bests = (np.abs(a) - np.abs(a).min(axis=1)[:,None]) == 0\n",
    "    best_last_time = np.argmax(bests.sum(axis=0))\n",
    "    combined_estimate.append(preds[i, best_last_time])\n",
    "combined_estimate = np.squeeze(combined_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds, combined_estimate, truth = fin.clip(preds, combined_estimate, truth)\n",
    "s = slice(1500, 1600)\n",
    "plt.plot(truth[s], 'k');\n",
    "plt.plot(preds[s], 'r', alpha=0.25)\n",
    "plt.plot(combined_estimate[s], 'r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heights = list((errors**2).mean(axis=0)) + [(combined_estimate**2).mean()]\n",
    "x = list(np.arange(errors.shape[1] + 1))\n",
    "plt.bar(x, heights)\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"predictor\")\n",
    "plt.xticks(x, labels=[xx for xx in x[:-1]] + [\"combined\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.corrcoef(truth.T, combined_estimate[:,None].T)[0,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
