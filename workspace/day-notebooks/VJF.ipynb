{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "from tqdm.notebook import trange\n",
    "import torch\n",
    "from vjf.online import VJF\n",
    "from scipy.stats import special_ortho_group\n",
    "from adaptive_latents.transformer import DecoupledTransformer\n",
    "from adaptive_latents import ArrayWithTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "total_time = 500\n",
    "seconds_per_rotation = 1\n",
    "samples_per_second = 10\n",
    "\n",
    "theta = np.linspace(0, total_time/seconds_per_rotation*2*np.pi, total_time*samples_per_second)\n",
    "x = np.column_stack([np.cos(theta), np.sin(theta)])\n",
    "y = x @ special_ortho_group(dim=10, seed=rng).rvs(1)[:2]\n",
    "y = y + rng.normal(0, 0.1, size=y.shape)\n",
    "u = np.zeros((y.shape[0], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.plot(*x[:11].T, '.')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "xdim = x.shape[-1]\n",
    "udim = u.shape[-1]\n",
    "\n",
    "ydim = y.shape[-1]\n",
    "\n",
    "\n",
    "config=dict(\n",
    "    resume=False,\n",
    "    xdim=xdim,  # dimension of hidden state\n",
    "    ydim=ydim,  # dimension of observations\n",
    "    udim=1,  # dimension of control vector\n",
    "    Ydim=udim,\n",
    "    Udim=udim,\n",
    "    rdim=50,  # number of RBFs\n",
    "    hdim=100,  # number of MLP hidden units\n",
    "    lr=1e-3,  # learning rate\n",
    "    clip_gradients=5.0,\n",
    "    debug=False,\n",
    "    likelihood='gaussian',  # \n",
    "    system='rbf',\n",
    "    recognizer='mlp',\n",
    "    C=(None, True),  # loading matrix: (initial, estimate)\n",
    "    b=(None, True),  # bias: (initial, estimate)\n",
    "    A=(None, False),  # transition matrix if LDS\n",
    "    B=(np.zeros((xdim, udim)), False),  # interaction matrix\n",
    "    Q=(1.0, True),  # state noise\n",
    "    R=(1.0, True),  # observation noise\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mdl = VJF(config)\n",
    "\n",
    "ys = torch.from_numpy(y).float()\n",
    "us = torch.from_numpy(u).float()\n",
    "\n",
    "mu = torch.zeros(ys.shape[0], xdim)\n",
    "q = None  # current state\n",
    "\n",
    "for i in np.arange(ys.shape[0]):\n",
    "    q, _ = mdl.feed((ys[i:i+1], us[i:i+1]), q0=q)\n",
    "    mu[i,:], _ = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(*mu[-200:].detach().numpy().T)\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VJF_transformer(DecoupledTransformer):\n",
    "    base_algorithm = VJF\n",
    "    \n",
    "    def __init__(self, *, config=None, latent_d=6, take_U=False, input_streams=None, output_streams=None, log_level=None):\n",
    "        if input_streams is None:\n",
    "            input_streams = {0:'Y', 1:'U'} if take_U else {0:'Y'}\n",
    "        DecoupledTransformer.__init__(self=self, input_streams=input_streams, output_streams=output_streams, log_level=log_level)\n",
    "        self.take_U = take_U\n",
    "        self.latent_d = latent_d\n",
    "        config = config or {}\n",
    "        self.config = self.default_config_dict({'xdim': self.latent_d}) | config\n",
    "        self.last_seen = {}\n",
    "        self.vjf = None\n",
    "        self.q = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def default_config_dict(update=None):\n",
    "        default_config = dict(\n",
    "            resume=False,\n",
    "            # xdim=6,  # dimension of hidden state\n",
    "            udim=1,  # dimension of control vector\n",
    "            # Ydim=udim,  # possibly not necessary?\n",
    "            # Udim=udim,  # possibly not necessary?\n",
    "            rdim=50,  # number of RBFs\n",
    "            hdim=100,  # number of MLP hidden units\n",
    "            lr=1e-3,  # learning rate\n",
    "            clip_gradients=5.0,\n",
    "            debug=False,\n",
    "            likelihood='gaussian',  # \n",
    "            system='rbf',\n",
    "            recognizer='mlp',\n",
    "            C=(None, True),  # loading matrix: (initial, estimate)\n",
    "            b=(None, True),  # bias: (initial, estimate)\n",
    "            A=(None, False),  # transition matrix if LDS\n",
    "            Q=(1.0, True),  # state noise\n",
    "            R=(1.0, True),  # observation noise\n",
    "            random_seed=0,\n",
    "\n",
    "            # these depend on the input dimensions\n",
    "            # ydim=ydim,  # dimension of observations\n",
    "            # B=(np.zeros((xdim, udim)), False),  # interaction matrix\n",
    "        )\n",
    "\n",
    "        update = update if update is not None else {}\n",
    "        return default_config | update  # the | makes a copy of the original dict\n",
    "        \n",
    "        \n",
    "    def init_vjf(self, ydim, udim=1):\n",
    "        assert self.take_U or udim==1\n",
    "        \n",
    "        self.config.update({\n",
    "            'ydim': ydim, \n",
    "            'udim': udim,\n",
    "            'B':(np.zeros((self.config['xdim'], udim)), False),\n",
    "        })\n",
    "        \n",
    "        self.vjf = VJF(self.config)\n",
    "\n",
    "\n",
    "    def _partial_fit(self, data, stream):\n",
    "        if stream in self.input_streams:\n",
    "            self.last_seen[self.input_streams[stream]] = data\n",
    "            \n",
    "            if len(self.last_seen) == len(self.input_streams):\n",
    "                y = self.last_seen['Y']\n",
    "                if self.take_U:\n",
    "                    u = self.last_seen['U']\n",
    "                else:\n",
    "                    u = np.zeros((y.shape[0], 1))\n",
    "                    \n",
    "                if self.vjf is None:\n",
    "                    self.init_vjf(ydim=y.shape[-1], udim=u.shape[-1])\n",
    "                    \n",
    "                y = torch.from_numpy(y).float()\n",
    "                u = torch.from_numpy(u).float()\n",
    "                self.q, _ = self.vjf.feed((y, u), q0=self.q)\n",
    "    \n",
    "    def transform(self, data, stream=0, return_output_stream=False):\n",
    "        if self.q is not None:\n",
    "        # if self.input_streams[stream] == 'Y' and self.q is not None:\n",
    "            q0 =  self.q[0].detach().numpy()\n",
    "            data = ArrayWithTime.from_transformed_data(q0, data)\n",
    "        \n",
    "        return (data, stream) if return_output_stream else data\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return super().get_params(deep=deep) | dict(take_U=self.take_U, latent_d=self.latent_d, config=self.config)\n",
    "\n",
    "\n",
    "v = VJF_transformer(input_streams={0:'Y'}, latent_d=2)\n",
    "# v.test_if_api_compatible()\n",
    "ret = v.offline_run_on([y], show_tqdm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(*ret[-200:].T)\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run with log_pred_p evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # 'cuda' does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_normal_logpdf(mean, variance, sample):\n",
    "    mean = mean.flatten()\n",
    "    variance = variance.flatten()\n",
    "    sample = sample.flatten()\n",
    "    \n",
    "    assert len(mean) == len(variance) == len(sample), f\"inconsistent shape: {mean.shape}, {variance.shape}, {sample.shape}\"\n",
    "    \n",
    "    logprobs = []\n",
    "    for i in range(len(sample)):\n",
    "        x = sample[i]\n",
    "        m = mean[i]\n",
    "        v = variance[i]\n",
    "        logprobs.append(-0.5 * ((x - m) ** 2 / v + np.log(2 * np.pi * v)))\n",
    "    return sum(logprobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(q, S, rng):\n",
    "    filtering_mu, filtering_logvar = q\n",
    "    \n",
    "    mu_f = filtering_mu[0].detach().cpu().numpy().T\n",
    "    var_f = filtering_logvar[0].detach().exp().cpu().numpy().T\n",
    "    Sigma_f = np.eye(xdim) * var_f\n",
    "\n",
    "    x = multivariate_normal(mu_f.flatten(), Sigma_f).rvs(size=S, random_state=rng).astype(np.float32)\n",
    "    x = torch.from_numpy(x).to(device)\n",
    "    return x\n",
    "\n",
    "def step(x, mdl, rng):\n",
    "    x += mdl.system.velocity(x) + mdl.system.noise.var ** 0.5 * torch.from_numpy(rng.normal(size=x.shape))\n",
    "    return x\n",
    "\n",
    "def get_logprob_and_distance(mdl, x, y_true):\n",
    "    y_var = mdl.likelihood.logvar.detach().exp().cpu().numpy().T\n",
    "    \n",
    "    y_tilde = mdl.decoder(x).detach().cpu().numpy()\n",
    "    \n",
    "    sample_logprobs = [diagonal_normal_logpdf(y_est, y_var, y_true) for y_est in y_tilde]\n",
    "    logprob = logsumexp(sample_logprobs) - np.log(x.shape[0])\n",
    "    \n",
    "    distance = np.linalg.norm(y_tilde - y_true, axis=-1).mean()\n",
    "    \n",
    "    return logprob, distance\n",
    "\n",
    "def log_step(q, mdl, ys, t, rng, S=1000, T=10):\n",
    "    x = generate_samples(q, S, rng)\n",
    "    \n",
    "    logprobs = []\n",
    "    distances = []\n",
    "    for i in range(T):\n",
    "        if t + i < ys.shape[0]:\n",
    "            y_tprime = ys[t + i].cpu().numpy()\n",
    "        else:\n",
    "            y_tprime = ys[t].cpu().numpy() * np.nan\n",
    "\n",
    "        x = step(x, mdl, rng)\n",
    "        logprob, distance = get_logprob_and_distance(mdl, x, y_tprime)\n",
    "\n",
    "        logprobs.append(logprob)\n",
    "        distances.append(distance)\n",
    "\n",
    "\n",
    "    return logprobs, distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = VJF(config).to(device)\n",
    "\n",
    "ys = torch.from_numpy(y).float().to(device)\n",
    "us = torch.from_numpy(u).float().to(device)\n",
    "\n",
    "# could be None except for the logging functions\n",
    "q = torch.zeros(1, xdim, device=device), torch.zeros(1, xdim, device=device)\n",
    "\n",
    "\n",
    "logprobs = []\n",
    "distances = []\n",
    "mu2 = np.zeros((ys.shape[0], xdim))\n",
    "\n",
    "for t in trange(ys.shape[0]):\n",
    "    step_logprobs, step_distances = log_step(q, mdl, ys, t, rng, T=10)\n",
    "    logprobs.append(step_logprobs)\n",
    "    distances.append(step_distances)\n",
    "\n",
    "    y_t = ys[t].unsqueeze(0)\n",
    "    u_t = us[t].unsqueeze(0)\n",
    "    q, loss = mdl.feed((y_t, u_t), q)\n",
    "    \n",
    "    mu2[t] = q[0].detach().numpy()\n",
    "\n",
    "logprobs, distances = np.array(logprobs), np.array(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig, axs = plt.subplots(nrows=2)\n",
    "\n",
    "for i in range(logprobs.shape[-1]):\n",
    "    axs[0].plot(logprobs[:, i], label=f\"{i+1} step{'s' if i > 0 else ''} ahead\")\n",
    "    axs[1].plot(distances[:, i], label=f\"{i+1} step{'s' if i > 0 else ''} ahead\")\n",
    "    \n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"time\")\n",
    "\n",
    "axs[0].set_ylabel(\"log probability\")\n",
    "axs[1].set_ylabel(\"average prediction distance\")\n",
    "axs[1].legend(bbox_to_anchor=(1.01, 0.95))\n",
    "# plt.ylim([-300, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks that the randomness is actually controlled by SEED.\n",
    "# This method is obviously hacky, but I'm keeping it because the logprobs\n",
    "# and distances were inconsistent between runs before, despite seeding.\n",
    "\n",
    "vars = {}\n",
    "for var in ['seed', 'x', 'y', 'mu', 'ys', 'mu2', 'distances', 'logprobs']:\n",
    "    v = globals()[var]\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        v = v.detach().cpu().numpy()\n",
    "    vars[var] = v\n",
    "\n",
    "    s = f'/tmp/asdf_{var}'\n",
    "    try:\n",
    "        old_v = np.load(f\"{s}.npy\")\n",
    "    except FileNotFoundError:\n",
    "        old_v = None\n",
    "    np.save(s, v)\n",
    "\n",
    "    if old_v is not None:\n",
    "        same = np.shape(v) == np.shape(old_v) and np.nanmax((v-old_v)**2) == 0\n",
    "        print(f'{var}: {same}')\n",
    "    else:\n",
    "        print(f'{var}: NEW')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
