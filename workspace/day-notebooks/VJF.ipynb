{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "from tqdm.notebook import trange\n",
    "import torch\n",
    "from vjf import online\n",
    "from scipy.stats import special_ortho_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "total_time = 1000\n",
    "seconds_per_rotation = 1\n",
    "samples_per_second = 10\n",
    "\n",
    "theta = np.linspace(0, total_time/seconds_per_rotation*2*np.pi, total_time*samples_per_second)\n",
    "x = np.column_stack([np.cos(theta), np.sin(theta)])\n",
    "y = x @ special_ortho_group(dim=10, seed=rng).rvs(1)[:2]\n",
    "y = y + rng.normal(0, 0.1, size=y.shape)\n",
    "u = np.zeros((y.shape[0], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "xdim = x.shape[-1]\n",
    "ydim = y.shape[-1]\n",
    "udim = u.shape[-1]\n",
    "\n",
    "device = 'cpu' # 'cuda' does not work\n",
    "likelihood='gaussian' # Gaussian observation\n",
    "dynamics = 'rbf'  # RBF network dynamic model\n",
    "recognizer = \"mlp\"  # MLP recognitiom model\n",
    "rdim = 50  # number of RBFs\n",
    "hdim = 100  # number of MLP hidden units\n",
    "\n",
    "config=dict(\n",
    "    resume=False,\n",
    "    xdim=xdim,\n",
    "    ydim=ydim,\n",
    "    udim=udim,\n",
    "    Ydim=udim,\n",
    "    Udim=udim,\n",
    "    rdim=rdim,\n",
    "    hdim=hdim,\n",
    "    lr=1e-3,\n",
    "    clip_gradients=5.0,\n",
    "    debug=False,\n",
    "    likelihood=likelihood,  # \n",
    "    system=dynamics,\n",
    "    recognizer=recognizer,\n",
    "    random_seed=seed,\n",
    "    C=(None, True),  # loading matrix: (initial, estimate)\n",
    "    b=(None, True),  # bias: (initial, estimate)\n",
    "    A=(None, False),  # transition matrix if LDS\n",
    "    B=(np.zeros((xdim, udim)), False),  # interaction matrix\n",
    "    Q=(1.0, True),  # state noise\n",
    "    R=(1.0, True),  # observation noise\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mdl = online.VJF(config)\n",
    "\n",
    "ys = torch.from_numpy(y).float()\n",
    "us = torch.from_numpy(u).float()\n",
    "\n",
    "mu = torch.zeros(ys.shape[0], xdim)\n",
    "q = None  # current state\n",
    "\n",
    "for i in np.arange(ys.shape[0]):\n",
    "    q, _ = mdl.feed((ys[i:i+1], us[i:i+1]), q0=q)\n",
    "    mu[i,:], _ = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(*mu[-200:].detach().numpy().T)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run with log_pred_p evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_normal_logpdf(mean, variance, sample):\n",
    "    mean = mean.flatten()\n",
    "    variance = variance.flatten()\n",
    "    sample = sample.flatten()\n",
    "    \n",
    "    assert len(mean) == len(variance) == len(sample), f\"inconsistent shape: {mean.shape}, {variance.shape}, {sample.shape}\"\n",
    "    \n",
    "    logprobs = []\n",
    "    for i in range(len(sample)):\n",
    "        x = sample[i]\n",
    "        m = mean[i]\n",
    "        v = variance[i]\n",
    "        logprobs.append(-0.5 * ((x - m) ** 2 / v + np.log(2 * np.pi * v)))\n",
    "    return sum(logprobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(q, S, rng):\n",
    "    filtering_mu, filtering_logvar = q\n",
    "    \n",
    "    mu_f = filtering_mu[0].detach().cpu().numpy().T\n",
    "    var_f = filtering_logvar[0].detach().exp().cpu().numpy().T\n",
    "    Sigma_f = np.eye(xdim) * var_f\n",
    "\n",
    "    x = multivariate_normal(mu_f.flatten(), Sigma_f).rvs(size=S, random_state=rng).astype(np.float32)\n",
    "    x = torch.from_numpy(x).to(device)\n",
    "    return x\n",
    "\n",
    "def step(x, mdl, rng):\n",
    "    x += mdl.system.velocity(x) + mdl.system.noise.var ** 0.5 * torch.from_numpy(rng.normal(size=x.shape))\n",
    "    return x\n",
    "\n",
    "def get_logprob_and_distance(mdl, x, y_true):\n",
    "    y_var = mdl.likelihood.logvar.detach().exp().cpu().numpy().T\n",
    "    \n",
    "    y_tilde = mdl.decoder(x).detach().cpu().numpy()\n",
    "    \n",
    "    sample_logprobs = [diagonal_normal_logpdf(y_est, y_var, y_true) for y_est in y_tilde]\n",
    "    logprob = logsumexp(sample_logprobs) - np.log(x.shape[0])\n",
    "    \n",
    "    distance = np.linalg.norm(y_tilde - y_true, axis=-1).mean()\n",
    "    \n",
    "    return logprob, distance\n",
    "\n",
    "def log_step(q, mdl, ys, t, rng, S=1000, T=10):\n",
    "    x = generate_samples(q, S, rng)\n",
    "    \n",
    "    logprobs = []\n",
    "    distances = []\n",
    "    for i in range(T):\n",
    "        if t + i < ys.shape[0]:\n",
    "            y_tprime = ys[t + i].cpu().numpy()\n",
    "        else:\n",
    "            y_tprime = ys[t].cpu().numpy() * np.nan\n",
    "\n",
    "        x = step(x, mdl, rng)\n",
    "        logprob, distance = get_logprob_and_distance(mdl, x, y_tprime)\n",
    "\n",
    "        logprobs.append(logprob)\n",
    "        distances.append(distance)\n",
    "\n",
    "\n",
    "    return logprobs, distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = online.VJF(config).to(device)\n",
    "\n",
    "ys = torch.from_numpy(y).float().to(device)\n",
    "us = torch.from_numpy(u).float().to(device)\n",
    "\n",
    "# could be None except for the logging functions\n",
    "q = torch.zeros(1, xdim, device=device), torch.zeros(1, xdim, device=device)\n",
    "\n",
    "\n",
    "logprobs = []\n",
    "distances = []\n",
    "mu2 = np.zeros((ys.shape[0], xdim))\n",
    "\n",
    "for t in trange(ys.shape[0]):\n",
    "    step_logprobs, step_distances = log_step(q, mdl, ys, t, rng, T=10)\n",
    "    logprobs.append(step_logprobs)\n",
    "    distances.append(step_distances)\n",
    "\n",
    "    y_t = ys[t].unsqueeze(0)\n",
    "    u_t = us[t].unsqueeze(0)\n",
    "    q, loss = mdl.feed((y_t, u_t), q)\n",
    "    \n",
    "    mu2[t] = q[0].detach().numpy()\n",
    "\n",
    "logprobs, distances = np.array(logprobs), np.array(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig, axs = plt.subplots(nrows=2)\n",
    "\n",
    "for i in range(logprobs.shape[-1]):\n",
    "    axs[0].plot(logprobs[:, i], label=f\"{i+1} step{'s' if i > 0 else ''} ahead\")\n",
    "    axs[1].plot(distances[:, i], label=f\"{i+1} step{'s' if i > 0 else ''} ahead\")\n",
    "    \n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"time\")\n",
    "\n",
    "axs[0].set_ylabel(\"log probability\")\n",
    "axs[1].set_ylabel(\"average prediction distance\")\n",
    "axs[1].legend(bbox_to_anchor=(1.01, 0.95))\n",
    "# plt.ylim([-300, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks that the randomness is actually controlled by SEED.\n",
    "# This method is obviously hacky, but I'm keeping it because the logprobs\n",
    "# and distances were inconsistent between runs before, despite seeding.\n",
    "\n",
    "vars = {}\n",
    "for var in ['seed', 'x', 'y', 'mu', 'ys', 'mu2', 'distances', 'logprobs']:\n",
    "    v = globals()[var]\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        v = v.detach().cpu().numpy()\n",
    "    vars[var] = v\n",
    "\n",
    "    s = f'/tmp/asdf_{var}'\n",
    "    try:\n",
    "        old_v = np.load(f\"{s}.npy\")\n",
    "    except FileNotFoundError:\n",
    "        old_v = None\n",
    "    np.save(s, v)\n",
    "\n",
    "    if old_v is not None:\n",
    "        same = np.shape(v) == np.shape(old_v) and np.nanmax((v-old_v)**2) == 0\n",
    "        print(f'{var}: {same}')\n",
    "    else:\n",
    "        print(f'{var}: NEW')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
