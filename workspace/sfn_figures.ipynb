{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T01:55:34.888685Z",
     "start_time": "2024-09-30T01:55:32.836676Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "import adaptive_latents\n",
    "from adaptive_latents import (\n",
    "    AnimationManager, \n",
    "    Pipeline, \n",
    "    CenteringTransformer, \n",
    "    proSVD, \n",
    "    Bubblewrap, \n",
    "    VanillaOnlineRegressor, \n",
    "    NumpyTimedDataSource, \n",
    "    KernelSmoother, \n",
    "    Concatenator, \n",
    "    sjPCA, \n",
    "    ZScoringTransformer, \n",
    "    mmICA\n",
    ")\n",
    "import adaptive_latents.input_sources.datasets as datasets\n",
    "import adaptive_latents.plotting_functions as pf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from adaptive_latents.timed_data_source import ArrayWithTime\n",
    "from IPython.display import Video, Image, display\n",
    "\n",
    "from importlib import reload\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "rng = np.random.default_rng()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "f28131f5-d629-4514-ba02-8817071a8d6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T01:55:35.065659Z",
     "start_time": "2024-09-30T01:55:34.889780Z"
    }
   },
   "source": "!echo $(hostname) $(git rev-parse --short HEAD)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tycho 73ea4c6\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "d8f5deeb1fa025dc",
   "metadata": {},
   "source": [
    "\n",
    "## Prediction tables\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T01:57:52.995328Z",
     "start_time": "2024-09-30T01:55:35.069345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%time\n",
    "%matplotlib inline\n",
    "shortrun = hostname == 'tycho' and True\n",
    "\n",
    "\n",
    "d = datasets.Odoherty21Dataset(neural_lag=0, drop_third_coord=True)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "0: [neural_data] -> center -> smooth -> concat[0] -> 2\n",
    "1: [behavior] -> Zscore -> concat[1] -> 2\n",
    "2: prosvd -> jpca -> bubblewrap -> regression1['x']\n",
    "3: [behavior] -> regression1['y']\n",
    "4: [neural_data] -> center -> smooth -> prosvd -> regression2['y']\n",
    "5: joints\n",
    "\"\"\"\n",
    "\n",
    "streams = []\n",
    "streams.append((d.neural_data, 0))\n",
    "streams.append((d.beh_pos, 1))\n",
    "# 2 is reserved for the post-concatination pipeline\n",
    "streams.append((d.beh_pos, 3))\n",
    "streams.append((d.neural_data, 4))\n",
    "# 5 for the alpha to joint\n",
    "\n",
    "# this pipeline makes the latent space\n",
    "p1 = Pipeline([\n",
    "    CenteringTransformer(init_size=100, input_streams={0:'X'}, output_streams={0:0}),\n",
    "    KernelSmoother(tau=0.04/d.bin_width, input_streams={0:'X'}, output_streams={0:0}),\n",
    "    zst:=ZScoringTransformer(init_size=10/d.beh_pos.dt, freeze_after_init=False, input_streams={1:'X'}, output_streams={1:1}),\n",
    "    Concatenator(input_streams={0: 0, 1: 1}, output_streams={0:2, 1:2, 'skip':-1}),\n",
    "])\n",
    "\n",
    "pro=proSVD(k=6, init_size=100, input_streams={2:'X'}, output_streams={2:2})\n",
    "jpca=sjPCA(init_size=100, input_streams={2:'X'}, output_streams={2:2})\n",
    "ica=mmICA(init_size=100, input_streams={2:'X'}, output_streams={2:2})\n",
    "\n",
    "\n",
    "# this pipeline handles the prediction and regression\n",
    "bw = Bubblewrap(\n",
    "    num=1100,\n",
    "    M=500,\n",
    "    lam=1e-3,\n",
    "    nu=1e-3,\n",
    "    eps=1e-4,\n",
    "    step=1,\n",
    "    num_grad_q=1,\n",
    "    sigma_orig_adjustment=100, # 0\n",
    "    input_streams={2:'X'},\n",
    "    output_streams={2:2},\n",
    "    log_level=1,\n",
    ")\n",
    "\n",
    "alpha_to_beh_reg = VanillaOnlineRegressor(\n",
    "    input_streams={2:'X', 3:'Y'},\n",
    ")\n",
    "\n",
    "neural_only_reg_pipeline = Pipeline([\n",
    "    CenteringTransformer(**(p1.steps[0].get_params() | dict(input_streams={4:'X'}, output_streams={4:4}))),\n",
    "    KernelSmoother(**(p1.steps[1].get_params() | dict(input_streams={4:'X'}, output_streams={4:4}))),\n",
    "    proSVD(k=6, init_size=100, input_streams={4:'X'}, output_streams={4:4}),\n",
    "    alpha_to_neural_reg:=VanillaOnlineRegressor(\n",
    "        input_streams={2:'X', 4:'Y'},\n",
    "    )\n",
    "    \n",
    "], \n",
    "reroute_inputs=False\n",
    ")\n",
    "\n",
    "alpha_to_joint_latents_reg = VanillaOnlineRegressor(\n",
    "    input_streams={2:'X', 5:'Y'},\n",
    ")\n",
    "\n",
    "\n",
    "exit_time = 40 if shortrun else 600\n",
    "\n",
    "pbar = tqdm(total=exit_time)\n",
    "pro_latents = []\n",
    "jpca_latents = []\n",
    "ica_latents = []\n",
    "\n",
    "next_bubble_predictions = []\n",
    "beh_predictions = []\n",
    "neural_predictions = []\n",
    "joint_predictions = []\n",
    "\n",
    "beh_target = []\n",
    "neural_target = []\n",
    "joint_target = []\n",
    "\n",
    "\n",
    "for output, stream in p1.streaming_run_on(streams, return_output_stream=True):\n",
    "    # prosvd step\n",
    "    pro_output, pro_stream = pro.partial_fit_transform(output, stream, return_output_stream=True)\n",
    "    if pro_stream == 2 and np.isfinite(pro_output).all():\n",
    "        pro_latents.append(pro_output)\n",
    "\n",
    "    # sjpca step\n",
    "    jpca_output, jpca_stream = jpca.partial_fit_transform(pro_output, pro_stream, return_output_stream=True)\n",
    "    if jpca_stream == 2 and np.isfinite(jpca_output).all():\n",
    "        jpca_latents.append(jpca_output)\n",
    "\n",
    "    # ica step (not in main line)\n",
    "    ica_output, ica_stream = ica.partial_fit_transform(pro_output, pro_stream, return_output_stream=True)\n",
    "    if ica_stream == 2 and np.isfinite(ica_output).all():\n",
    "        ica_latents.append(ica_output)\n",
    "\n",
    "    # bw step\n",
    "    pre_bw_output, pre_bw_stream = jpca_output, jpca_stream\n",
    "    output, stream = bw.partial_fit_transform(pre_bw_output, pre_bw_stream, return_output_stream=True)\n",
    "\n",
    "    # fit all the regressions on alpha\n",
    "    alpha_to_beh_reg.partial_fit_transform(output, stream)\n",
    "    neural_stream_output, neural_stream_stream = neural_only_reg_pipeline.partial_fit_transform(output, stream, return_output_stream=True)\n",
    "    \n",
    "    alpha_to_joint_latents_reg.partial_fit_transform(output, stream)\n",
    "    alpha_to_joint_latents_reg.partial_fit_transform(pre_bw_output, 5 if pre_bw_stream == 2 else pre_bw_stream)\n",
    "    \n",
    "    if stream == 3:\n",
    "        beh_target.append(output)\n",
    "        \n",
    "    if neural_stream_stream == 4 and not np.isnan(neural_stream_output).any():\n",
    "        neural_target.append(neural_stream_output)\n",
    "        \n",
    "    if pre_bw_stream == 2:\n",
    "        joint_target.append(output)\n",
    "    \n",
    "    if stream == 2 and np.isfinite(output).all(): # do predictions\n",
    "        prediction_t = output.t + bw.dt\n",
    "\n",
    "        # alpha_pred = bw.get_alpha_at_t(0, relative_t=True)\n",
    "\n",
    "        alpha_pred = bw.get_alpha_at_t(prediction_t)\n",
    "\n",
    "        # alpha_pred[np.argmax(bw.alpha)] = 0\n",
    "        # alpha_pred[alpha_pred < alpha_pred.max()] = 0\n",
    "        # alpha_pred = alpha_pred / alpha_pred.sum()\n",
    "\n",
    "\n",
    "        next_bubble_predictions.append(ArrayWithTime(bw.mu[np.argmax(alpha_pred)], t=prediction_t))\n",
    "        \n",
    "        beh_predictions.append(ArrayWithTime(alpha_to_beh_reg.predict(alpha_pred), t=prediction_t))\n",
    "        neural_predictions.append(ArrayWithTime(alpha_to_neural_reg.predict(alpha_pred), t=prediction_t))\n",
    "        joint_predictions.append(ArrayWithTime(alpha_to_joint_latents_reg.predict(alpha_pred), t=prediction_t))\n",
    "\n",
    "    if output.t >= exit_time:\n",
    "        break\n",
    "\n",
    "    pbar.update(round(output.t,1) - pbar.n)\n",
    "\n",
    "\n",
    "pro_latents = ArrayWithTime.from_list(pro_latents)\n",
    "jpca_latents = ArrayWithTime.from_list(jpca_latents)\n",
    "ica_latents = ArrayWithTime.from_list(ica_latents)\n",
    "\n",
    "next_bubble_predictions = ArrayWithTime.from_list(next_bubble_predictions)\n",
    "\n",
    "beh_predictions = ArrayWithTime.from_list(beh_predictions)\n",
    "neural_predictions = ArrayWithTime.from_list(neural_predictions)\n",
    "joint_predictions = ArrayWithTime.from_list(joint_predictions)\n",
    "\n",
    "\n",
    "beh_target = ArrayWithTime.from_list(beh_target)\n",
    "neural_target = ArrayWithTime.from_list(neural_target)\n",
    "joint_target = ArrayWithTime.from_list(joint_target)\n",
    "\n"
   ],
   "id": "7b9730f2918f2be6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A newer version (0.63.1) of dandi/dandi-cli is available. You are using 0.63.0\n",
      "/home/jgould/miniconda3/envs/adaptive_latents/lib/python3.9/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.0 because version 1.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/jgould/miniconda3/envs/adaptive_latents/lib/python3.9/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'core' version 2.4.0 because version 2.7.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n",
      "/home/jgould/miniconda3/envs/adaptive_latents/lib/python3.9/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.1.0 because version 0.5.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b572c123f74d44b0b928dabb471930e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgould/miniconda3/envs/adaptive_latents/lib/python3.9/site-packages/numba/core/typed_passes.py:336: NumbaPerformanceWarning: \u001B[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001B[1m\n",
      "File \"../../../miniconda3/envs/adaptive_latents/lib/python3.9/site-packages/mmica/_densities.py\", line 44:\u001B[0m\n",
      "\u001B[1m@jit(parallel=True, fastmath=True, nopython=True)\n",
      "\u001B[1mdef logp_u(Y):\n",
      "\u001B[0m\u001B[1m^\u001B[0m\u001B[0m\n",
      "\u001B[0m\n",
      "  warnings.warn(errors.NumbaPerformanceWarning(msg,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 99\u001B[0m\n\u001B[1;32m     96\u001B[0m     pro_latents\u001B[38;5;241m.\u001B[39mappend(pro_output)\n\u001B[1;32m     98\u001B[0m \u001B[38;5;66;03m# sjpca step\u001B[39;00m\n\u001B[0;32m---> 99\u001B[0m jpca_output, jpca_stream \u001B[38;5;241m=\u001B[39m \u001B[43mjpca\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartial_fit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpro_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpro_stream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_output_stream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m jpca_stream \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39misfinite(jpca_output)\u001B[38;5;241m.\u001B[39mall():\n\u001B[1;32m    101\u001B[0m     jpca_latents\u001B[38;5;241m.\u001B[39mappend(jpca_output)\n",
      "File \u001B[0;32m~/Documents/AdaptiveLatents/adaptive_latents/transformer.py:182\u001B[0m, in \u001B[0;36mDecoupledTransformer.partial_fit_transform\u001B[0;34m(self, data, stream, return_output_stream)\u001B[0m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpartial_fit_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m, data, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, return_output_stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartial_fit(data, stream)\n\u001B[0;32m--> 182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_output_stream\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AdaptiveLatents/adaptive_latents/transformer.py:323\u001B[0m, in \u001B[0;36mTypicalTransformer.transform\u001B[0;34m(self, data, stream, return_output_stream)\u001B[0m\n\u001B[1;32m    321\u001B[0m         data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnan \u001B[38;5;241m*\u001B[39m data\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 323\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform_for_X\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_streams[stream]\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_output_stream:\n",
      "File \u001B[0;32m~/Documents/AdaptiveLatents/adaptive_latents/jpca.py:128\u001B[0m, in \u001B[0;36msjPCA.transform_for_X\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform_for_X\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproject\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/AdaptiveLatents/adaptive_latents/jpca.py:97\u001B[0m, in \u001B[0;36mBaseSJPCA.project\u001B[0;34m(self, x, project_up)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mproject\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, project_up\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m---> 97\u001B[0m     U \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_U\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x \u001B[38;5;241m@\u001B[39m (U \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m project_up \u001B[38;5;28;01melse\u001B[39;00m U\u001B[38;5;241m.\u001B[39mT)\n",
      "File \u001B[0;32m~/Documents/AdaptiveLatents/adaptive_latents/jpca.py:64\u001B[0m, in \u001B[0;36mBaseSJPCA.get_U\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     62\u001B[0m U[:, i \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreal(u1)\n\u001B[1;32m     63\u001B[0m U[:, i\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreal(u2)\n\u001B[0;32m---> 64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_U \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39mall(\u001B[38;5;241;43m~\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misnan\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlast_U\u001B[49m\u001B[43m)\u001B[49m):\n\u001B[1;32m     65\u001B[0m     U[:, (i \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m):(i\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m)], _ \u001B[38;5;241m=\u001B[39m align_column_spaces(U[:, (i \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m):(i\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m)], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_U[:, (i \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m):(i\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m)])\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;66;03m# TODO: also permute planes?\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "6a0702a1-820a-4fc1-ac50-a4b46180cf27",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "behavior_dicts = [\n",
    "    dict(\n",
    "        true_behavior=beh_target,\n",
    "        true_behavior_t=beh_target.t,\n",
    "        predicted_behavior=beh_predictions,\n",
    "        predicted_behavior_t=beh_predictions.t,\n",
    "        label='beh'\n",
    "    ),\n",
    "    dict(\n",
    "        true_behavior=neural_target,\n",
    "        true_behavior_t=neural_target.t,\n",
    "        predicted_behavior=neural_predictions,\n",
    "        predicted_behavior_t=neural_predictions.t,\n",
    "        label='neural'\n",
    "    ),\n",
    "    dict(\n",
    "        true_behavior=joint_target,\n",
    "        true_behavior_t=joint_target.t,\n",
    "        predicted_behavior=joint_predictions,\n",
    "        predicted_behavior_t=joint_predictions.t,\n",
    "        label='joint'\n",
    "    ),\n",
    "    ]\n",
    "adaptive_latents.plotting_functions = reload(adaptive_latents.plotting_functions)\n",
    "adaptive_latents.plotting_functions.plot_bw_pipeline([bw], behavior_dicts)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.matshow(bw.log['alpha'][-300:])\n",
    "ax.set_title('alpha')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83f768b31ba6cccc",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, squeeze=False, layout='tight', figsize=(10,10))\n",
    "\n",
    "\n",
    "for idx, latents in enumerate([pro_latents, jpca_latents, ica_latents]):\n",
    "    ax = axs[idx, 0]\n",
    "    # ax.scatter(latents[:,0], latents[:,1], s=5)\n",
    "\n",
    "    d_latents = np.diff(latents, axis=0)\n",
    "    d_latents = d_latents / np.linalg.norm(d_latents, axis=1)[:, np.newaxis]\n",
    "    ax.quiver(latents[:-1, 0], latents[:-1, 1], d_latents[:, 0], d_latents[:, 1], scale=1 / 20, units='dots')\n",
    "    x1, x2, y1, y2 = ax.axis()\n",
    "    x_points = np.linspace(x1, x2, 10)\n",
    "    y_points = np.linspace(y1, y2, 10)\n",
    "    mgrid = np.meshgrid(*[np.linspace(latents[:, i].min(), latents[:, i].max(), 10) for i in range(latents.shape[1])])\n",
    "    \n",
    "    ax.axis('equal')\n",
    "\n",
    "    ax = axs[idx, 1]\n",
    "\n",
    "\n",
    "e1, e2 = np.zeros(6), np.zeros(6)\n",
    "e1[0] = 1\n",
    "e2[1] = 1\n",
    "\n",
    "axs[0,1].plot(pro.inverse_transform(e1, 2))\n",
    "axs[0,1].plot(pro.inverse_transform(e2, 2))\n",
    "axs[0,0].set_ylabel('pro')\n",
    "\n",
    "axs[1,1].plot(pro.inverse_transform(jpca.inverse_transform(e1, 2), 2))\n",
    "axs[1,1].plot(pro.inverse_transform(jpca.inverse_transform(e2, 2), 2))\n",
    "axs[1,0].set_ylabel('jpca')\n",
    "\n",
    "axs[2,1].plot(pro.inverse_transform(ica.inverse_transform(e1, 2), 2))\n",
    "axs[2,1].plot(pro.inverse_transform(ica.inverse_transform(e2, 2), 2))\n",
    "axs[2,0].set_ylabel('ica')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df6296055eabbf3e",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "latents = pro_latents\n",
    "\n",
    "dim_1, dim_2 = 0,1\n",
    "pf.plot_history_with_tail(ax, latents, dim_1=dim_1, dim_2=dim_2)\n",
    "means = np.mean(latents, axis=0)\n",
    "\n",
    "ax.arrow(latents[-1,dim_1], latents[-1,dim_2], means[dim_1] - latents[-1,dim_1], means[dim_2] - latents[-1,dim_2], zorder=5, head_width=.1, color='k')\n",
    "\n",
    "ax.axis('equal');\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a3edfd94e83a13c",
   "metadata": {},
   "source": [
    "assert isinstance(d, datasets.Zong22Dataset)\n",
    "desired_pattern = np.mean(latents[-101:-1], axis=0)\n",
    "current_pattern = latents[-1]\n",
    "desired_stim = desired_pattern - current_pattern\n",
    "\n",
    "desired_pattern = (desired_stim @ pro.Q.T)[:d.neural_data.a.shape[-1]]\n",
    "\n",
    "near_zero = np.abs(desired_stim) < 100\n",
    "print(near_zero.sum())\n",
    "desired_stim[near_zero] = np.nan\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "d.show_stim_pattern(ax, np.abs(desired_pattern))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "847f7b4cbad6dccf",
   "metadata": {},
   "source": [
    "## Intro video"
   ]
  },
  {
   "cell_type": "code",
   "id": "3a6ce2c9dee00ac2",
   "metadata": {},
   "source": [
    "d = datasets.Odoherty21Dataset()\n",
    "\n",
    "tail_length = 5  # seconds\n",
    "\n",
    "with AnimationManager(n_cols=2, n_rows=1, figsize=(10, 5)) as am:\n",
    "    for current_t in np.linspace(100, 110, 51):\n",
    "        am.axs[0,0].cla()\n",
    "\n",
    "        n_columns = np.floor(tail_length / d.bin_width).astype(int)\n",
    "        idx = np.nonzero(~(d.neural_data.t < current_t))[0][0]\n",
    "        \n",
    "        \n",
    "        am.axs[0,0].imshow(d.neural_data.a[idx-n_columns:idx,0,:].T, aspect='auto', interpolation='none', extent=[current_t - tail_length, current_t, d.neural_data.a.shape[2], 0])\n",
    "\n",
    "\n",
    "\n",
    "        old_lims = am.axs[0,1].axis()\n",
    "        am.axs[0,1].cla()\n",
    "        am.axs[0,1].axis('off')\n",
    "        \n",
    "        s = ((current_t - tail_length) < d.behavioral_data.t) & (d.behavioral_data.t < current_t)\n",
    "        am.axs[0,1].plot(d.behavioral_data.a[s,0,0], d.behavioral_data.a[s,0,1])\n",
    "        pf.use_bigger_lims(am.axs[0,1], old_lims)\n",
    "        \n",
    "        am.grab_frame()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
